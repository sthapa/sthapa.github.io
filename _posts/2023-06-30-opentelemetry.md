---
title: "OpenTelemetry in SLATE"
overview: Adding OpenTelemetry to an existing code base
published: true
permalink: blog/2023-06-30-opentelemetry.html
attribution: Suchandra Thapa
layout: default
type: markdown
---

A post about what OpenTelemetry is and how to use it.  There's examples of
using auto-instrumentation for python and manually instrumenting C++ code.

<!--end_excerpt-->

## Background

[OpenTelemetry](https://opentelemetry.io/) is a collection of tools and SDKs
that allow developers to collect information about their applications' runtime
metrics. OpenTelemetry collects information about duration required to handle
user requests and about error rates when processing requests. OpenTelemetry can
correlate and combine this information across multiple services in order to
generate a unified view of user interactions.

OpenTelemetry is primarily known for collecting statistics about the RED (Rate,
Errors, Duration) for operations.  These three numbers provide a lot of
information about services: **rate** shows how frequently an function is 
called, **errors** shows how often an error occurs when a function is called,
and **duration** shows how long a function call takes.  OpenTelemetry can 
instrument applications down to individual function calls or even things 
like a single SQL query.  

Combined with aggregate statistics like P50, P90, P90 for function calls, 
administrators can gain a lot of insight into things like where errors occur
or what is latency is contributing to anomalous latency.

We instrumented the [SLATE](https://slateci.io) services with OpenTelemetry in
order to better track user interactions and to help with debugging errors that
may occur.

## OpenTelemetry

OpenTelemetry is an [observability
framework](https://opentelemetry.io/docs/concepts/observability-primer/#what-is-observability)
that encompasses a variety of tools and SDKs that combine to record 
end-to-end interactions even if the interaction spans multiple
services such as web portals, API services, and database calls.

Generally, OpenTelemetry is used by instrumenting applications to collect
telemetry data (traces, metrics, logs). This information is then sent to a
collector that stores the data in a time series database. Finally, there is a
frontend that presents information. 

OpenTelemetry provides multiple ways to instrument services or applications 
in order to collect telemetry information. The most powerful but also 
the most time consuming is to use the various language specific libraries to
emit telemetry data at various points during execution.  The easiest is to
use auto-instrumentation to automatically generate telemetry data to services
written in popular languages like python, java, ruby, or javascript. This 
article describes using both approaches: manual instrumentation for c++ and 
auto-instrumentation for python.

### Traces and Spans

[Traces](https://opentelemetry.io/docs/concepts/signals/traces/) are the
primary unit of measurement that OpenTelemetry is concerned about. Traces are
intended to record all activity that occurs in a single user interaction.
Traces are usually broken down into atomic units of work called **spans**. A
**span** might consist of something like a SQL query run against a database, a
call to a microservice, or an operation on a storage device.

Traces are usually generated by a **Trace Provider** that is integrated into a
service as an SDK. There are providers for multiple languages such as Python,
C++, Javascript, Java, etc. In SLATE, we use the C++ and Python providers.

Trace providers will:
1. Aggregate **spans** created in an application
1. Bundle the aggregate **spans** into traces
1. Send them to a collector for further processing

Trace providers also provide facilities that allow information about traces
to be passed around and incorporated into an existing trace. This allows 
interactions that span multiple services to be linked together.

### Collectors

OpenTelemetry uses collectors to receive and process traces from trace
providers. A collector provides a centralized location for collecting traces
from multiple trace providers and combines related traces so that interactions
across different services and sources can be associated with a single user
interaction.

Collectors can also apply additional processing to traces it receives and store
processed traces in persistent storage (usually a time series database like
[Clickhouse](https://clickhouse.com/)).  Collectors can do things like 
sampling traces (to reduce storage needs) as well as discarding or keeping traces
based on criteria like trace duration or error conditions.  For example, a
collector may sample 10% of the traces that succeed and keep any traces that
include an error or which exceed a given duration. 

### Signoz

We chose to use [Signoz](https://signoz.io/) to handle the duties of storing
and presenting traces. Signoz is an open source platform for presenting
OpenTelemetry data and provides:
* A Helm chart for deploying an OpenTelemetry collector and Clickhouse database
  pair to store traces, metrics, and logs.
* Alerting and monitoring of services based on traces received.
* A web frontend to view and query traces, metrics, or logs that have been collected.

## Instrumenting SLATE

### C++

The SLATE API server is written in C++. In order to instrument this component,
we incorporated the [OpenTelemetry C++
client](https://github.com/open-telemetry/opentelemetry-cpp) to generate and
send traces. Although the process was a bit tedious, it was relatively
straightforward.  Unfortunately, OpenTelemetry doesn't offer any way to 
auto-instrument C++ code so this was the best option.

The core of the OpenTelemetry code is located in
[Telemetry.cpp](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp).
The `initializeTracer` function is called when the server starts up. It takes
the configuration settings for the server and initializes a trace provider for
the API server with the appropriate collector, sampling parameters, and other
settings.  

Also of note is the `HttpTextMapCarrier` class that checks HTTP headers for incoming
requests for headers that indicate that the API call is part of an existing trace.
This is used in `getWebSpanOptions` function along with a `propagator` object to
add trace information to any generated traces and spans so that they can be
linked the existing trace on the calling service.

Within each function involved in handling incoming API calls, the code obtains
the trace provider using
[getTracer](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L124).
This gets a `shared_ptr` to a tracer object that can generate **spans**
associated with handling an incoming API call.

1. If the function is directly handling an incoming API request (e.g. the web
framework routes incoming HTTP requests to this function), it will then use
[setWebSpanAttributes](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L130)
and
[getWebSpanOptions](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L152)
to get attributes and options for the **span**.
1. These options and attributes are then passed to the `StartSpan` method of
the tracer in order to create a new **span** that will cover the work done by
this function.
1.
[populateSpan](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L179)
is called right after the **span** is generated to add various information
(client IP, HTTP method, etc.) about the incoming HTTP request to the **span**.
1. If an error occurs within the function,
[setWebSpanError](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L209)
is used to populate the **span** with error information to aid in debugging.
1. Finally, the **span**'s `End()` method is used to close the **span** and
send it to the OpenTelemetry collector.

If the function that is being run is not directly processing an incoming API
call, it does something slightly different to generate a **span**.

1. The
[getTracer](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L124)
function is still called to get a `shared_ptr` to a tracer object.
1.
[setInternalSpanAttributes](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L167)
and
[getInternalSpanOptions](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L172)
are used to get the options and attributes for the **span**.
1. These are then used when creating a new **span** using the `StartSpan`
method of the tracer object.
1. If an error occurs during the function call,
[setSpanError](https://github.com/slateci/slate-client-server/blob/master/src/Telemetry.cpp#L215)
is used to set the appropriate fields in the **span** to aid in debugging.
1. Finally, the **span**'s `End()` method is called just before the function
exits.

### Python

The SLATE Portal uses Python and
[Flask](https://flask.palletsprojects.com/en/2.3.x/) to provide a web interface
for SLATE. Unlike with C++, OpenTelemetry provides an operator to auto-instrument
Python and Flask code so traces are automatically generated. This is
achieved by deploying an instrumentation CRD with the SLATE Portal
Kubernetes pods.

1. We create a CRD that sets the trace endpoints as well as the
auto-instrumentation that should be used.
   
   ```yaml
   apiVersion: opentelemetry.io/v1alpha1
   kind: Instrumentation
   metadata:
     name: slate-instrumentation
   spec:
     exporter:
       endpoint: http://injection-collector-collector.development.svc.cluster.local:4318
     propagators:
       - tracecontext
       - baggage
       - b3
     python:
       image: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-python:latest
   ```
   
1. We update the labels for the Kubernetes pod that runs the Portal with
   annotations that indicate that an OpenTelemetry sidecar should be deployed.
   
   ```yaml
   metadata:
     annotations:
       sidecar.opentelemetry.io/inject: "injection-collector"
       instrumentation.opentelemetry.io/inject-python: "true"
   ```

1. The OpenTelemetry operator will then generate a sidecar for the pod running 
   the Flask endpoint and inject the sidecar into the running pod.  The sidecar
   will automatically generate traces for each API call to the Flask endpoint and
   send it to the the collector described bellow.
   
1. The OpenTelemetry operator used CRD describe in 1 to automatically deploy 
   a collector in the same Kubernetes namespace as the Portal pods. This 
   collector is used to collect traces from the Portal and forward them to a 
   central collector.
   
   {% raw %}
   ```yaml
   apiVersion: opentelemetry.io/v1alpha1
   kind: OpenTelemetryCollector
   metadata:
     name: injection-collector
   spec:
     config: |
       receivers:
         otlp:
           protocols:
             grpc:
             http:
       processors:
         memory_limiter:
           check_interval: 1s
           limit_percentage: 75
           spike_limit_percentage: 15
         batch:
           send_batch_size: 10000
           timeout: 10s
       exporters:
         logging:
         otlphttp:
           endpoint: opentel.collector.dns
       service:
         pipelines:
           traces:
             receivers: [otlp]
             processors: []
             exporters: [logging, otlphttp]
   ```
   {% endraw %}
   **Note:** Vertically scroll in the code-block above to view the entire YAML expression.

## Observability

Using Signoz, we can:

* Examine operations on the SLATE Portal or within the SLATE API server.
* Search for interactions based on search criteria like *username*, the
  *cluster* being worked on, errors, HTTP codes (e.g. `200`, `500`, `403`,
  etc.) as well as time taken to handle API calls.

In short, Signoz allows us to find anomalous calls taking more time than usual
or to find API calls that result in elevated error rates (e.g. due to a problem
with a SLATE cluster).

## Adding monitoring

Signoz also allows us to automatically send alerts to team Slack channels when
incoming traces indicate that certain API calls result in elevated error rates
or require significantly more time than usual to process a request. This allows
us to proactively investigate potential issues.

##  Conclusion

Although adding OpenTelemetry to the SLATE infrastructure required large
changes to our codebase and to our infrastructure, the resulting improvements
in observability and debugging drastically improved our ability to monitor and
respond to problems within the SLATE infrastructure.
